DM: in eval, *significance*/*implications* of results are hard to glean.
a) are these e2e applications workloads that would tend to emphasize
   COWL's overhead, or deemphasize it? (Answer: *emphasize* it, i.e.,
   harsh workloads for COWL, because the simple apps illustrate partitionings,
   which means they have almost no compute in each compartment, and lots of
   crossings between compartments. Real-world apps would have way more compute
   within each compartment, so COWL's overhead would decrease as a fraction
   of total compute.
b) what are the take-aways from the numbers? what's the good news, and what's
   our worst news? (Answer: could say 16% overhead on some operation as
   highest overhead. And remind people this is a very harsh workload per
   above. And could say that overall, e2e completion times change
   imperceptibly for users.)
concrete suggestions:
a) state at *start* of section why these workloads are harsh for COWL's
   overhead (explanation above)
b) maybe a bullet or two summarizing two take-aways from numbers at end
   of "preamble" to eval section.

DM/Mihai: need to make clear that apps ran over loopback i/f. That, too,
means the results of these experiments emphasize COWL's overhead; in a real
network environment with, say, several tens of ms of RTT, COWL's overhead
will become a much smaller fraction of e2e latency.

3. Table 2: some unexpected speedups are not explained, such as "New
iframe - Firefox - labeled" and "New worker - Firefox - unlabeled[c]."

We will update the explanation of the table to note that this
difference (14.5 ms unlabeled, 14.4 ms labeled) is small enough to be
explained by variability in the distribution of completion times.
